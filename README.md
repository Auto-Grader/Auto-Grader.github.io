# GitHub Actions Auto-grader (Bubble Sort) — Repo Template

This document contains a ready-to-use repository layout, **simulated student submissions**, **10 teacher test cases** (from easy → hard), and a **GitHub Actions workflow** that runs the grading locally on GitHub Actions and produces per-student reports + an aggregate CSV.

> **Important:** This repo executes untrusted student code inside GitHub-hosted runners. For real student data, follow your institution security policy. This example is for testing/education only.

---

## Repo structure

```
/ (repo root)
├─ submissions/
│  ├─ s01.py
│  ├─ s02.py
│  ├─ s03.py
│  └─ s04.py
├─ tests/
│  ├─ test_cases.py
│  ├─ exec_test.py
│  └─ grade_all.py
├─ reports/    # generated by grading run
├─ results.csv  # generated by grading run
└─ .github/
   └─ workflows/
      └─ grade.yml
```

---

## README / Interface (expected)

The assignment expects a Python function with the following signature placed in the student file (e.g. `s01.py`):

```python
# expected file: submissions/s01.py

def bubble_sort(arr):
    """
    Sort `arr` in-place using bubble sort algorithm. This function may either:
      - modify `arr` in place and return `None`, OR
      - return a sorted list (in which case the harness will check the return value).

    The harness will test correctness by comparing to Python's sorted() result.
    """
    ...
```

**Please follow this contract** — the runner tolerates both in-place or return semantics.

---

## Simulated student submissions

Four example students are provided in `submissions/`:

* `s01.py` — correct in-place bubble sort
* `s02.py` — returns a new sorted list (still acceptable)
* `s03.py` — buggy (fails for reverse-sorted input)
* `s04.py` — raises an exception for certain inputs

(See file contents below.)

---

## Tests: `tests/test_cases.py`

This file defines 10 tests. Each test entry contains:

* `name` — short test id
* `input` — input list
* `expected` — expected sorted list
* `points` — points awarded for fully passing this test
* `description` — detailed grading criteria and common failure reasons

The tests escalate from trivial (empty list) to harder cases (large random list, repeated values, negative numbers, performance edge-case).

---

## Test harness: `tests/exec_test.py` and `tests/grade_all.py`

* `exec_test.py` runs a single test *in a separate process* (so import-time side-effects in student modules are isolated). It loads the specified student's file, calls `bubble_sort`, times execution, and prints a JSON result to stdout.

* `grade_all.py` iterates all `submissions/*.py`, runs each of the 10 tests (via `exec_test.py` subprocess calls with a per-test timeout), aggregates results, writes `reports/{student}.json`, and produces `results.csv` with columns:
  `student_id,total_score,max_score,passed_tests,failed_tests,report_path`

The grader assigns exactly the `points` value per test (no partial credit in this template), and appends human-readable `error_reason` per failed test into the JSON report.

---

## GitHub Actions workflow: `.github/workflows/grade.yml`

This workflow does the following on `push` and on manual dispatch:

1. Check out the repo
2. Set up Python
3. Run `python3 tests/grade_all.py`
4. Upload `reports/` and `results.csv` as workflow artifacts
5. *(optional — commented)* Commit `results.csv` back to the repo using `GITHUB_TOKEN`

Artifacts are downloadable from the workflow run page.

---

## Full file listing (copy each into your repo)

### `submissions/s01.py` (correct in-place implementation)

```python
# submissions/s01.py

def bubble_sort(arr):
    # In-place bubble sort
    n = len(arr)
    for i in range(n):
        swapped = False
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swapped = True
        if not swapped:
            break
    return None


if __name__ == '__main__':
    a = [5, 2, 9, 1]
    bubble_sort(a)
    print(a)
```

### `submissions/s02.py` (returns a new sorted list)

```python
# submissions/s02.py

def bubble_sort(arr):
    # Incorrect semantics if teacher insisted in-place, but we accept return
    b = arr.copy()
    n = len(b)
    for i in range(n):
        for j in range(0, n - i - 1):
            if b[j] > b[j + 1]:
                b[j], b[j + 1] = b[j + 1], b[j]
    return b


if __name__ == '__main__':
    a = [3, 4, 1]
    print(bubble_sort(a))
```

### `submissions/s03.py` (bug: doesn't handle reversed lists correctly)

```python
# submissions/s03.py

def bubble_sort(arr):
    # Bug: loop bounds wrong (skips last swap when reverse sorted)
    n = len(arr)
    for i in range(n - 1):
        for j in range(0, n - i - 2):  # <-- off-by-one
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return None


if __name__ == '__main__':
    a = [5, 4, 3, 2, 1]
    bubble_sort(a)
    print(a)
```

### `submissions/s04.py` (raises exception for non-list)

```python
# submissions/s04.py

def bubble_sort(arr):
    # Not robust: will raise if arr contains None or uncomparable types
    n = len(arr)
    for i in range(n):
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return None


if __name__ == '__main__':
    a = [1, None, 2]
    bubble_sort(a)
    print(a)
```

---

### `tests/test_cases.py` (10 tests with descriptions)

```python
# tests/test_cases.py

TESTS = [
    {
        'name': 'empty_list',
        'input': [],
        'expected': [],
        'points': 10,
        'description': 'Empty list must remain empty. Common errors: throwing on len=0 or index errors.'
    },
    {
        'name': 'single_element',
        'input': [1],
        'expected': [1],
        'points': 10,
        'description': 'Single-element lists should be unchanged.'
    },
    {
        'name': 'already_sorted',
        'input': [1, 2, 3, 4],
        'expected': [1, 2, 3, 4],
        'points': 10,
        'description': 'Stable behavior on already sorted lists; ensure no out-of-bounds access.'
    },
    {
        'name': 'reverse_sorted',
        'input': [5, 4, 3, 2, 1],
        'expected': [1, 2, 3, 4, 5],
        'points': 10,
        'description': 'Reverse-sorted input is a common edge-case. Off-by-one errors often fail here.'
    },
    {
        'name': 'duplicates',
        'input': [2, 3, 2, 1, 3],
        'expected': [1, 2, 2, 3, 3],
        'points': 10,
        'description': 'Ensure duplicates are handled correctly and algorithm is stable (optional).'
    },
    {
        'name': 'negative_numbers',
        'input': [-1, 5, -3, 2],
        'expected': [-3, -1, 2, 5],
        'points': 10,
        'description': 'Sorting must handle negative numbers and mixed sign values.'
    },
    {
        'name': 'strings_order',
        'input': ['b', 'a', 'aa', ''],
        'expected': ['', 'a', 'aa', 'b'],
        'points': 10,
        'description': 'If asked to support general comparable elements, test string ordering. Some solutions assume numeric lists only.'
    },
    {
        'name': 'large_random_small',
        'input': list(range(50, 0, -1)),
        'expected': list(range(1, 51)),
        'points': 10,
        'description': 'Short performance / correctness check. Bubble sort should still finish quickly for 50 elements but check for timeouts in heavy implementations.'
    },
    {
        'name': 'mixed_types_failure',
        'input': [1, 'a', 2],
        'expected': None,
        'points': 10,
        'description': 'Mixed incomparable types should raise an exception or be handled. Accept either a raised TypeError or a documented behavior. For this assignment, we expect an exception (test passes if exception raised).'
    },
    {
        'name': 'inplace_vs_return',
        'input': [3, 1, 2],
        'expected': [1, 2, 3],
        'points': 10,
        'description': 'Accept either in-place modification (function returns None) or returning a sorted list. The final result must equal sorted(input).'
    }
]

MAX_SCORE = sum(t['points'] for t in TESTS)
```

---

### `tests/exec_test.py` (executes one test in a child process)

```python
# tests/exec_test.py

import argparse
import json
import importlib.util
import sys
import time
import traceback
from tests.test_cases import TESTS


def load_and_run(submission_path, test_input, timeout_sec=5):
    """
    Load the student's module from path and call `bubble_sort` on a copy of test_input.
    Returns a dict with fields: passed(bool), error_reason(str|None), output(value), time_ms, exception
    """
    start = time.time()
    try:
        spec = importlib.util.spec_from_file_location('student_module', submission_path)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
    except Exception as e:
        return {
            'passed': False,
            'error_reason': 'ImportError: ' + repr(e),
            'output': None,
            'time_ms': int((time.time() - start) * 1000),
            'exception': traceback.format_exc()
        }

    if not hasattr(mod, 'bubble_sort'):
        return {
            'passed': False,
            'error_reason': 'No function bubble_sort(arr) found in submission',
            'output': None,
            'time_ms': int((time.time() - start) * 1000),
            'exception': None
        }

    func = getattr(mod, 'bubble_sort')

    import copy
    arr = copy.deepcopy(test_input)

    try:
        t0 = time.time()
        result = func(arr)
        t1 = time.time()
        time_ms = int((t1 - t0) * 1000)
    except Exception as e:
        return {
            'passed': False,
            'error_reason': 'RuntimeError: ' + repr(e),
            'output': None,
            'time_ms': int((time.time() - start) * 1000),
            'exception': traceback.format_exc()
        }

    # Determine final array
    final = None
    if result is None:
        final = arr
    else:
        final = result

    return {
        'passed': final == test_input_sorted(test_input),
        'error_reason': None if final == test_input_sorted(test_input) else f'Output mismatch. Got={final}, Expected={test_input_sorted(test_input)}',
        'output': final,
        'time_ms': time_ms,
        'exception': None
    }


def test_input_sorted(x):
    # For tests where expected is None (mixed types), we can't call sorted(); handled elsewhere.
    try:
        return sorted(x)
    except Exception:
        return None


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--submission', required=True)
    parser.add_argument('--test-index', type=int, required=True)
    args = parser.parse_args()

    test = TESTS[args.test_index]
    test_input = test['input']

    result = load_and_run(args.submission, test_input)
    # For test where expected is None (mixed types), we consider test passed if exception raised
    if test['expected'] is None:
        if result.get('exception'):
            result['passed'] = True
            result['error_reason'] = None
        else:
            # If no exception raised, but sorting produced something, mark as failed
            result['passed'] = False
            result['error_reason'] = 'Expected TypeError or similar for mixed types, but function returned successfully.'

    # Add metadata
    result['test_name'] = test['name']
    result['points'] = test['points']

    print(json.dumps(result))
```

---

### `tests/grade_all.py` (aggregates all tests across all submissions)

```python
# tests/grade_all.py

import glob
import json
import os
import subprocess
from tests.test_cases import TESTS, MAX_SCORE

REPORT_DIR = 'reports'
os.makedirs(REPORT_DIR, exist_ok=True)

submissions = sorted(glob.glob('submissions/*.py'))

rows = []

for sub in submissions:
    student_id = os.path.basename(sub).replace('.py', '')
    report = {
        'student_id': student_id,
        'submission': sub,
        'tests': [],
        'total_score': 0,
        'max_score': MAX_SCORE
    }

    for idx, t in enumerate(TESTS):
        try:
            completed = subprocess.run(
                ['python3', 'tests/exec_test.py', '--submission', sub, '--test-index', str(idx)],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=8,
                text=True
            )
        except subprocess.TimeoutExpired:
            out = json.dumps({
                'passed': False,
                'error_reason': 'TimeoutExceeded',
                'output': None,
                'time_ms': None,
                'exception': 'Timeout'
            })
            res = json.loads(out)
            res['test_name'] = t['name']
            res['points'] = t['points']
        else:
            if completed.returncode != 0 and completed.stdout.strip() == '':
                # Something crashed hard — include stderr
                res = {
                    'passed': False,
                    'error_reason': 'Execution failed',
                    'output': None,
                    'time_ms': None,
                    'exception': completed.stderr
                }
                res['test_name'] = t['name']
                res['points'] = t['points']
            else:
                try:
                    res = json.loads(completed.stdout)
                except Exception as e:
                    res = {
                        'passed': False,
                        'error_reason': 'Malformed runner output',
                        'output': completed.stdout,
                        'time_ms': None,
                        'exception': completed.stderr
                    }
                    res['test_name'] = t['name']
                    res['points'] = t['points']

        # scoring
        if res.get('passed'):
            report['total_score'] += t['points']
        else:
            # attach human readable failure reason
            if res.get('error_reason') is None:
                res['error_reason'] = 'Unknown failure'

        report['tests'].append(res)

    # write report
    report_path = os.path.join(REPORT_DIR, f'{student_id}.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    passed = sum(1 for x in report['tests'] if x.get('passed'))
    failed = len(report['tests']) - passed

    rows.append({
        'student_id': student_id,
        'total_score': report['total_score'],
        'max_score': report['max_score'],
        'passed_tests': passed,
        'failed_tests': failed,
        'report_path': report_path
    })

# write CSV
import csv
with open('results.csv', 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['student_id', 'total_score', 'max_score', 'passed_tests', 'failed_tests', 'report_path']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()
    for r in rows:
        writer.writerow(r)

print('Grading finished. Results saved to results.csv and reports/ per student.')
```

---

### `.github/workflows/grade.yml` (GitHub Actions workflow)

```yaml
name: Auto-grade Submissions

on:
  push:
    paths:
      - 'submissions/**'
      - 'tests/**'
  workflow_dispatch: {}

jobs:
  grade:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # no extra deps for this example

      - name: Run grader
        run: |
          python3 tests/grade_all.py

      - name: Upload reports artifact
        uses: actions/upload-artifact@v4
        with:
          name: grading-reports
          path: |
            reports/
            results.csv

      # Optional: commit results.csv back to repository (uncomment if you want to persist)
      # - name: Commit results
      #   uses: stefanzweifel/git-auto-commit-action@v4
      #   with:
      #     commit_message: 'Auto-commit grading results'
      #     file_pattern: 'results.csv'
      #     branch: 'main'
      #     # The default GITHUB_TOKEN is used by this action to push
```

---

## How to use

1. Commit and push all student submissions. The workflow will run (on push) and generate artifacts.
2. Open the workflow run → `Artifacts` → download `grading-reports` to retrieve `results.csv` and per-student JSON reports.

If you want the CSV to appear in the repo automatically, enable the optional "Commit results" step.

---

## TODO: Next steps / customization ideas

* Add partial credit scoring (e.g. 50% if algorithm works but is not in-place when required).
* Add static analysis to detect cheating or forbidden constructs (e.g. calls to `sorted()` when not allowed).
* Expand language support (wrap compile+run steps for C/Java).
* Replace raw subprocess execution with containerized execution if you want stricter isolation.
